{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a5262b0-9118-41df-a6c6-a4fa003f59ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "import torch\n",
    "from torchtext import data\n",
    "import re\n",
    "from transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from collections import defaultdict\n",
    "from textwrap import wrap\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from gensim import corpora\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from collections import Counter\n",
    "# alt.data_transformers.enable('data_server')\n",
    "# from sklearn.preprocessing import MinMaxScaler, StandardScaler, OrdinalEncoder, OneHotEncoder\n",
    "# from sklearn.pipeline import Pipeline, make_pipeline\n",
    "# from sklearn.model_selection import train_test_split, cross_val_score, cross_validate\n",
    "# from sklearn.compose import ColumnTransformer\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.dummy import DummyClassifier\n",
    "# from sklearn.utils import resample\n",
    "# from sklearn.metrics import roc_curve\n",
    "# from sklearn.metrics import auc\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "# from imblearn.under_sampling import RandomUnderSampler\n",
    "# from imblearn.pipeline import Pipeline as imbpipeline\n",
    "# from sklearn.metrics import f1_score as f1\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# import torch\n",
    "# from torch import nn\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from torch.utils.data import TensorDataset\n",
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "# import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf39dd1e",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6240ae5-43f9-45af-978d-9678856795ad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Text</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands and i will murder yo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Positive</td>\n",
       "      <td>I am coming to the borders and I will kill you...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands and i will kill you ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Positive</td>\n",
       "      <td>im coming on borderlands and i will murder you...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands 2 and i will murder ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Sentiment                                               Text  Target\n",
       "0  Positive  im getting on borderlands and i will murder yo...       1\n",
       "1  Positive  I am coming to the borders and I will kill you...       1\n",
       "2  Positive  im getting on borderlands and i will kill you ...       1\n",
       "3  Positive  im coming on borderlands and i will murder you...       1\n",
       "4  Positive  im getting on borderlands 2 and i will murder ...       1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"twitter_training.csv\", header = None)\n",
    "df = df.drop([0, 1], axis = 1)\n",
    "df = df.applymap(str)\n",
    "df.columns = [\"Sentiment\", \"Text\"]\n",
    "df[\"Target\"] = df[\"Sentiment\"].map({\"Negative\" : 0, \"Positive\" : 1, \"Neutral\" : 2, \"Irrelevant\" : 3})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0aa1a50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_eng = stopwords.words('english')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    link_re_pattern = \"https?:\\/\\/t.co/[\\w]+\"\n",
    "    punct = \"[^\\w\\s]+\"\n",
    "    text = re.sub(link_re_pattern, \"\", text)\n",
    "    text = re.sub(punct, \"\", text)\n",
    "    return text.lower()\n",
    "\n",
    "def build_tweet_corpus(data):\n",
    "\n",
    "    word_counter = build_tweet_counter(data)\n",
    "    ls = []\n",
    "    for key in word_counter:\n",
    "        if word_counter[key] < 5:\n",
    "            ls.append(key)\n",
    "    \n",
    "    stopwords = set(stopwords_eng + ls)\n",
    "    tweet_corpus = set()\n",
    "    for x in data:\n",
    "        for word in x:\n",
    "            if word not in stopwords:\n",
    "                tweet_corpus.add(word)\n",
    "    return list(tweet_corpus)\n",
    "\n",
    "def build_tweet_counter(data):\n",
    "    \n",
    "    tweet_corpus = []\n",
    "    for x in data:\n",
    "        for word in x:\n",
    "            if word not in stopwords_eng:\n",
    "                tweet_corpus.append(word)\n",
    "    return Counter(tweet_corpus)\n",
    "\n",
    "\n",
    "def process_data(df):\n",
    "\n",
    "    tokenizer = TweetTokenizer()\n",
    "\n",
    "    df[\"Text\"] = df[\"Text\"].apply(preprocess_text)\n",
    "    df[\"Tokens\"] = df[\"Text\"].apply(tokenizer.tokenize)\n",
    "    df[\"Tokens\"] = df[\"Tokens\"].apply(lambda x: [word for word in x if word not in stopwords_eng])\n",
    "\n",
    "    return df\n",
    "\n",
    "def tokenize(df, tweet_corpus, max_len):\n",
    "    corpus_dict = corpora.Dictionary([tweet_corpus]).token2id\n",
    "    \n",
    "    def to_tokenids(text):\n",
    "        tokens = [corpus_dict[x] for x in text if x in corpus_dict]\n",
    "        if len(tokens) <= 1:\n",
    "            return \"NA\"\n",
    "        else:\n",
    "            return np.array(tokens)\n",
    "\n",
    "    df[\"Tokens\"] = df[\"Tokens\"].apply(to_tokenids)\n",
    "    df = df[df[\"Tokens\"] != \"NA\"]\n",
    "    lens = torch.LongTensor([len(x) for x in df[\"Tokens\"]])\n",
    "\n",
    "    def pad(x):\n",
    "        if len(x) < max_len:\n",
    "            x = np.append(x, [0]*(max_len - len(x)))\n",
    "        return x[0:max_len]\n",
    "\n",
    "    df['Tokens'] = df[\"Tokens\"].apply(pad)\n",
    "    return df, lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14a32714",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed = process_data(df)\n",
    "traindata, valdata = train_test_split(df, test_size = 0.2, random_state= 321)\n",
    "\n",
    "# length = df_processed[\"Tokens\"].apply(len)\n",
    "# plt.hist(length, bins = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19b23e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\andy8\\anaconda3\\envs\\stock_trade\\lib\\site-packages\\pandas\\core\\ops\\array_ops.py:75: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  result = libops.scalar_compare(x.ravel(), y, op)\n",
      "C:\\Users\\andy8\\AppData\\Local\\Temp\\ipykernel_24756\\3237871160.py:65: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Tokens'] = df[\"Tokens\"].apply(pad)\n"
     ]
    }
   ],
   "source": [
    "df_processed = process_data(df)\n",
    "traindata, valdata = train_test_split(df, test_size = 0.2, random_state= 321)\n",
    "\n",
    "train_corpus = build_tweet_corpus(df[\"Tokens\"])\n",
    "\n",
    "max_len = 35\n",
    "traindata, trainlens = tokenize(traindata, train_corpus, 35)\n",
    "valdata, vallens = tokenize(valdata, train_corpus, 35)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f06d7358",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = np.stack(traindata[\"Tokens\"])\n",
    "trainY = np.array(traindata[\"Target\"])\n",
    "validX = np.stack(valdata[\"Tokens\"])\n",
    "validY = np.array(valdata[\"Target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "217170a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torchtrain = TensorDataset(torch.from_numpy(trainX).to(torch.int64), torch.from_numpy(trainY).to(torch.int64))\n",
    "torchtval = TensorDataset(torch.from_numpy(validX).to(torch.int64), torch.from_numpy(validY).to(torch.int64))\n",
    "\n",
    "trainloader = DataLoader(torchtrain, shuffle=True, batch_size=16)\n",
    "validloader = DataLoader(torchtval, shuffle=True, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "900d7c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentRNN(nn.Module):\n",
    "    def __init__(self, voab_size, embedding_dim, hidden_dim, layers):\n",
    "        super(SentimentRNN, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(voab_size, embedding_dim)\n",
    "        self.out_dim = 1\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layers = layers\n",
    "\n",
    "        self.rnn = nn.RNN(input_size = embedding_dim, hidden_size = hidden_dim, num_layers = layers, batch_first = True)\n",
    "    \n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.linear = nn.Linear(hidden_dim, 4)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        batch_size = input.size(0)\n",
    "        lengths = 35 - (input == 0).sum(dim=1)\n",
    "        lengths.to(device)\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "        embeds = self.embedding(input)\n",
    "        \n",
    "        embeds = nn.utils.rnn.pack_padded_sequence(embeds, list(lengths), batch_first=True, enforce_sorted=False)\n",
    "        rnn_out, hidden = self.rnn(embeds, hidden)\n",
    "        out = self.linear(hidden[-1, :, :])\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        hidden = torch.zeros((self.layers, batch_size, self.hidden_dim)).to(device)\n",
    "        return hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 35])\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(trainloader)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "lr=0.0001\n",
    "no_layers = 3\n",
    "vocab_size = len(train_corpus)\n",
    "embedding_dim = 32\n",
    "hidden_dim = 128\n",
    "\n",
    "model = SentimentRNN(vocab_size, embedding_dim, hidden_dim, no_layers)\n",
    "model.to(device)\n",
    "\n",
    "#moving to gpu\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# device = \"cpu\"\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "optimizer.step()\n",
    "\n",
    "inputs, labels = next(dataiter)\n",
    "labels = labels.to(torch.int64)\n",
    "inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "ip = inputs[0]\n",
    "ip = ip[None, :]\n",
    "print(ip.shape)\n",
    "output = model(ip)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "279c2c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=0.002\n",
    "no_layers = 2\n",
    "vocab_size = len(train_corpus)\n",
    "embedding_dim = 64\n",
    "hidden_dim = 128\n",
    "\n",
    "model = SentimentRNN(vocab_size, embedding_dim, hidden_dim, no_layers)\n",
    "\n",
    "#moving to gpu\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# device = \"cpu\"\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# function to predict accuracy\n",
    "def acc(pred,label):\n",
    "    pred = torch.round(pred.squeeze())\n",
    "    return torch.sum(pred == label.squeeze()).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8c622c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "train_loss : 1.234100037334559\n",
      "train_accuracy : 45.271880415944544\n",
      "valid accuracy: 57.31812933025404\n",
      "Epoch 2\n",
      "train_loss : 0.9228681534709509\n",
      "train_accuracy : 64.34503177354131\n",
      "valid accuracy: 65.27857967667437\n",
      "Epoch 3\n",
      "train_loss : 0.7258017960503258\n",
      "train_accuracy : 73.46187175043327\n",
      "valid accuracy: 71.36258660508084\n",
      "Epoch 4\n",
      "train_loss : 0.6216010637495264\n",
      "train_accuracy : 78.01487579433854\n",
      "valid accuracy: 72.47401847575058\n",
      "Epoch 5\n",
      "train_loss : 0.5408907866892031\n",
      "train_accuracy : 81.0532206816869\n",
      "valid accuracy: 76.08978060046189\n",
      "Epoch 6\n",
      "train_loss : 0.48726429907009994\n",
      "train_accuracy : 83.08239456961294\n",
      "valid accuracy: 77.4032909930716\n",
      "Epoch 7\n",
      "train_loss : 0.4650562313250263\n",
      "train_accuracy : 83.9092287694974\n",
      "valid accuracy: 76.85479214780601\n",
      "Epoch 8\n",
      "train_loss : 0.45276361737337883\n",
      "train_accuracy : 84.32986712882726\n",
      "valid accuracy: 77.55484988452656\n",
      "Epoch 9\n",
      "train_loss : 0.4459589270574639\n",
      "train_accuracy : 84.54289428076257\n",
      "valid accuracy: 76.6743648960739\n",
      "Epoch 10\n",
      "train_loss : 0.45811250837598316\n",
      "train_accuracy : 84.16377816291161\n",
      "valid accuracy: 74.87009237875289\n",
      "Epoch 11\n",
      "train_loss : 0.4568738917239626\n",
      "train_accuracy : 84.15475158867707\n",
      "valid accuracy: 77.19399538106235\n",
      "Epoch 12\n",
      "train_loss : 0.4854273409531562\n",
      "train_accuracy : 82.80618139803582\n",
      "valid accuracy: 74.98556581986143\n",
      "Epoch 13\n",
      "train_loss : 0.5046781386926141\n",
      "train_accuracy : 82.1616839976892\n",
      "valid accuracy: 74.37933025404158\n",
      "Epoch 14\n",
      "train_loss : 0.5306386259841237\n",
      "train_accuracy : 81.0369728480647\n",
      "valid accuracy: 74.07621247113164\n",
      "Epoch 15\n",
      "train_loss : 0.5330650436480495\n",
      "train_accuracy : 80.90879549393414\n",
      "valid accuracy: 72.92147806004618\n",
      "Epoch 16\n",
      "train_loss : 0.561029454751305\n",
      "train_accuracy : 79.81116406701328\n",
      "valid accuracy: 73.52771362586606\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 28\u001b[0m\n\u001b[0;32m     26\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     27\u001b[0m train_losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m---> 28\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m preds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(output, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     30\u001b[0m corr \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (preds \u001b[38;5;241m==\u001b[39m labels)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\andy8\\anaconda3\\envs\\stock_trade\\lib\\site-packages\\torch\\optim\\optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    368\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    369\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    370\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    371\u001b[0m             )\n\u001b[1;32m--> 373\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    376\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\andy8\\anaconda3\\envs\\stock_trade\\lib\\site-packages\\torch\\optim\\optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32mc:\\Users\\andy8\\anaconda3\\envs\\stock_trade\\lib\\site-packages\\torch\\optim\\adam.py:163\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    152\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    155\u001b[0m         group,\n\u001b[0;32m    156\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    160\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    161\u001b[0m         state_steps)\n\u001b[1;32m--> 163\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\andy8\\anaconda3\\envs\\stock_trade\\lib\\site-packages\\torch\\optim\\adam.py:311\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    309\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 311\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    313\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    323\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    324\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    325\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    326\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    327\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 15\n",
    "valid_loss_min = np.Inf\n",
    "batch_size = 16\n",
    "\n",
    "epoch_tr_loss, epoch_vl_loss = [],[]\n",
    "epoch_tr_acc, epoch_vl_acc = [],[]\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_losses = []\n",
    "    train_acc = 0.0\n",
    "    corr = 0\n",
    "    tot = 0\n",
    "\n",
    "    corrval = 0\n",
    "    totval = 0\n",
    "    model.train()\n",
    "    # initialize hidden state \n",
    "    for inputs, labels in trainloader:\n",
    "        labels = labels.to(torch.int64)\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        model.zero_grad()\n",
    "\n",
    "        output = model(inputs)\n",
    "        # calculate the loss and perform backprop\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        train_losses.append(loss.item())\n",
    "        optimizer.step()\n",
    "        preds = torch.argmax(output, 1)\n",
    "        corr += (preds == labels).sum().item()\n",
    "        tot += 16\n",
    "\n",
    "    for inputs, labels in validloader:\n",
    "        labels = labels.to(torch.int64)\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        output = model(inputs)\n",
    "        epoch_val_loss = criterion(output, labels)\n",
    "        # calculate the loss and perform backprop\n",
    "        preds = torch.argmax(output, 1)\n",
    "        corrval += (preds == labels).sum().item()\n",
    "        totval += 16\n",
    "\n",
    "    epoch_train_loss = np.mean(train_losses)\n",
    "    print(f'Epoch {epoch+1}')\n",
    "    print(f'train_loss : {epoch_train_loss}')\n",
    "    print(f'train_accuracy : {corr/tot*100}')\n",
    "    print(f\"valid accuracy: {corrval/totval*100}\")\n",
    "    \n",
    "    if epoch_val_loss <= valid_loss_min:\n",
    "        torch.save(model.state_dict(), '../working/state_dict.pt')\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,epoch_val_loss))\n",
    "        valid_loss_min = epoch_val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b70006be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\andy8\\anaconda3\\envs\\stock_trade\\lib\\site-packages\\pandas\\core\\ops\\array_ops.py:75: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  result = libops.scalar_compare(x.ravel(), y, op)\n",
      "C:\\Users\\andy8\\AppData\\Local\\Temp\\ipykernel_7992\\3237871160.py:65: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Tokens'] = df[\"Tokens\"].apply(pad)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test accuacy is: 0.7876227614095899\n"
     ]
    }
   ],
   "source": [
    "testdf = pd.read_csv(\"twitter_validation.csv\", header = None)\n",
    "testdf = testdf.drop([0, 1], axis = 1)\n",
    "dtestdff = testdf.applymap(str)\n",
    "testdf.columns = [\"Sentiment\", \"Text\"]\n",
    "testdf[\"Target\"] = testdf[\"Sentiment\"].map({\"Negative\" : 0, \"Positive\" : 1, \"Neutral\" : 2, \"Irrelevant\" : 3})\n",
    "\n",
    "df_processed_test = process_data(testdf)\n",
    "\n",
    "max_len = 35\n",
    "testdata, testlens = tokenize(df_processed_test, train_corpus, 35)\n",
    "\n",
    "testX = np.stack(testdata[\"Tokens\"])\n",
    "testY = np.array(testdata[\"Target\"])\n",
    "torchtest = TensorDataset(torch.from_numpy(trainX).to(torch.int64), torch.from_numpy(trainY).to(torch.int64))\n",
    "\n",
    "testloader = DataLoader(torchtest, shuffle=True, batch_size=16)\n",
    "\n",
    "corrval = 0\n",
    "totval = 0\n",
    "\n",
    "for inputs, labels in testloader:\n",
    "    labels = labels.to(torch.int64)\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "    output = model(inputs)\n",
    "    # calculate the loss and perform backprop\n",
    "    preds = torch.argmax(output, 1)\n",
    "    corrval += (preds == labels).sum().item()\n",
    "    totval += 16\n",
    "\n",
    "print(f\"The test accuacy is: {corrval/totval}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8279553e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(sentence, model, tweet_corpus, max_len):\n",
    "    sentence = sentence.lower()\n",
    "    words = sentence.split(\" \")\n",
    "    corpus_dict = corpora.Dictionary([tweet_corpus]).token2id\n",
    "    tokens = [corpus_dict[x] for x in words if x in corpus_dict]\n",
    "    if len(tokens) <= 1:\n",
    "        print(\"No Valid Strings!\")\n",
    "        return None\n",
    "    else:\n",
    "        tokens = np.array(tokens)\n",
    "\n",
    "    if len(tokens) < max_len:\n",
    "        tokens = np.append(tokens, [0]*(max_len - len(tokens)))\n",
    "        tokens = tokens[0:max_len]\n",
    "    \n",
    "    tokens = torch.LongTensor(tokens[None, :])\n",
    "    tokens = tokens.to(device)\n",
    "    output = model(tokens)\n",
    "    preds = torch.argmax(output, 1)\n",
    "    return preds\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1], device='cuda:0')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(\"I like cheese\", model, train_corpus, 35)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1279a53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stock_trade",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
