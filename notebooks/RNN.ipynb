{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a5262b0-9118-41df-a6c6-a4fa003f59ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "import torch\n",
    "from torchtext import data\n",
    "import re\n",
    "from transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from collections import defaultdict\n",
    "from textwrap import wrap\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from gensim import corpora\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf39dd1e",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6240ae5-43f9-45af-978d-9678856795ad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Text</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands and i will murder yo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Positive</td>\n",
       "      <td>I am coming to the borders and I will kill you...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands and i will kill you ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Positive</td>\n",
       "      <td>im coming on borderlands and i will murder you...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands 2 and i will murder ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Sentiment                                               Text  Target\n",
       "0  Positive  im getting on borderlands and i will murder yo...       1\n",
       "1  Positive  I am coming to the borders and I will kill you...       1\n",
       "2  Positive  im getting on borderlands and i will kill you ...       1\n",
       "3  Positive  im coming on borderlands and i will murder you...       1\n",
       "4  Positive  im getting on borderlands 2 and i will murder ...       1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"twitter_training.csv\", header = None)\n",
    "df = df.drop([0, 1], axis = 1)\n",
    "df = df.applymap(str)\n",
    "df.columns = [\"Sentiment\", \"Text\"]\n",
    "df[\"Target\"] = df[\"Sentiment\"].map({\"Negative\" : 0, \"Positive\" : 1, \"Neutral\" : 2, \"Irrelevant\" : 3})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0aa1a50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_eng = stopwords.words('english')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    link_re_pattern = \"https?:\\/\\/t.co/[\\w]+\"\n",
    "    punct = \"[^\\w\\s]+\"\n",
    "    text = re.sub(link_re_pattern, \"\", text)\n",
    "    text = re.sub(punct, \"\", text)\n",
    "    return text.lower()\n",
    "\n",
    "def build_tweet_corpus(data):\n",
    "\n",
    "    word_counter = build_tweet_counter(data)\n",
    "    ls = []\n",
    "    for key in word_counter:\n",
    "        if word_counter[key] < 5:\n",
    "            ls.append(key)\n",
    "    \n",
    "    stopwords = set(stopwords_eng + ls)\n",
    "    tweet_corpus = set()\n",
    "    for x in data:\n",
    "        for word in x:\n",
    "            if word not in stopwords:\n",
    "                tweet_corpus.add(word)\n",
    "    return list(tweet_corpus)\n",
    "\n",
    "def build_tweet_counter(data):\n",
    "    \n",
    "    tweet_corpus = []\n",
    "    for x in data:\n",
    "        for word in x:\n",
    "            if word not in stopwords_eng:\n",
    "                tweet_corpus.append(word)\n",
    "    return Counter(tweet_corpus)\n",
    "\n",
    "\n",
    "def process_data(df):\n",
    "\n",
    "    tokenizer = TweetTokenizer()\n",
    "\n",
    "    df[\"Text\"] = df[\"Text\"].apply(preprocess_text)\n",
    "    df[\"Tokens\"] = df[\"Text\"].apply(tokenizer.tokenize)\n",
    "    df[\"Tokens\"] = df[\"Tokens\"].apply(lambda x: [word for word in x if word not in stopwords_eng])\n",
    "\n",
    "    return df\n",
    "\n",
    "def tokenize(df, tweet_corpus, max_len):\n",
    "    corpus_dict = corpora.Dictionary([tweet_corpus]).token2id\n",
    "    \n",
    "    def to_tokenids(text):\n",
    "        tokens = [corpus_dict[x] for x in text if x in corpus_dict]\n",
    "        if len(tokens) <= 1:\n",
    "            return \"NA\"\n",
    "        else:\n",
    "            return np.array(tokens)\n",
    "\n",
    "    df[\"Tokens\"] = df[\"Tokens\"].apply(to_tokenids)\n",
    "    df = df[df[\"Tokens\"] != \"NA\"]\n",
    "    lens = torch.LongTensor([len(x) for x in df[\"Tokens\"]])\n",
    "\n",
    "    def pad(x):\n",
    "        if len(x) < max_len:\n",
    "            x = np.append(x, [0]*(max_len - len(x)))\n",
    "        return x[0:max_len]\n",
    "\n",
    "    df['Tokens'] = df[\"Tokens\"].apply(pad)\n",
    "    return df, lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14a32714",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed = process_data(df)\n",
    "traindata, valdata = train_test_split(df, test_size = 0.2, random_state= 321)\n",
    "\n",
    "# length = df_processed[\"Tokens\"].apply(len)\n",
    "# plt.hist(length, bins = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19b23e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\andy8\\anaconda3\\envs\\stock_trade\\lib\\site-packages\\pandas\\core\\ops\\array_ops.py:75: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  result = libops.scalar_compare(x.ravel(), y, op)\n",
      "C:\\Users\\andy8\\AppData\\Local\\Temp\\ipykernel_24756\\3237871160.py:65: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Tokens'] = df[\"Tokens\"].apply(pad)\n"
     ]
    }
   ],
   "source": [
    "df_processed = process_data(df)\n",
    "traindata, valdata = train_test_split(df, test_size = 0.2, random_state= 321)\n",
    "\n",
    "train_corpus = build_tweet_corpus(df[\"Tokens\"])\n",
    "\n",
    "max_len = 35\n",
    "traindata, trainlens = tokenize(traindata, train_corpus, 35)\n",
    "valdata, vallens = tokenize(valdata, train_corpus, 35)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f06d7358",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = np.stack(traindata[\"Tokens\"])\n",
    "trainY = np.array(traindata[\"Target\"])\n",
    "validX = np.stack(valdata[\"Tokens\"])\n",
    "validY = np.array(valdata[\"Target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "217170a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torchtrain = TensorDataset(torch.from_numpy(trainX).to(torch.int64), torch.from_numpy(trainY).to(torch.int64))\n",
    "torchtval = TensorDataset(torch.from_numpy(validX).to(torch.int64), torch.from_numpy(validY).to(torch.int64))\n",
    "\n",
    "trainloader = DataLoader(torchtrain, shuffle=True, batch_size=16)\n",
    "validloader = DataLoader(torchtval, shuffle=True, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "900d7c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentRNN(nn.Module):\n",
    "    def __init__(self, voab_size, embedding_dim, hidden_dim, layers):\n",
    "        super(SentimentRNN, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(voab_size, embedding_dim)\n",
    "        self.out_dim = 1\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layers = layers\n",
    "\n",
    "        self.rnn = nn.RNN(input_size = embedding_dim, hidden_size = hidden_dim, num_layers = layers, batch_first = True)\n",
    "    \n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.linear = nn.Linear(hidden_dim, 4)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        batch_size = input.size(0)\n",
    "        lengths = 35 - (input == 0).sum(dim=1)\n",
    "        lengths.to(device)\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "        embeds = self.embedding(input)\n",
    "        \n",
    "        embeds = nn.utils.rnn.pack_padded_sequence(embeds, list(lengths), batch_first=True, enforce_sorted=False)\n",
    "        rnn_out, hidden = self.rnn(embeds, hidden)\n",
    "        out = self.linear(hidden[-1, :, :])\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        hidden = torch.zeros((self.layers, batch_size, self.hidden_dim)).to(device)\n",
    "        return hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 35])\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(trainloader)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "lr=0.0001\n",
    "no_layers = 3\n",
    "vocab_size = len(train_corpus)\n",
    "embedding_dim = 32\n",
    "hidden_dim = 128\n",
    "\n",
    "model = SentimentRNN(vocab_size, embedding_dim, hidden_dim, no_layers)\n",
    "model.to(device)\n",
    "\n",
    "#moving to gpu\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# device = \"cpu\"\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "optimizer.step()\n",
    "\n",
    "inputs, labels = next(dataiter)\n",
    "labels = labels.to(torch.int64)\n",
    "inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "ip = inputs[0]\n",
    "ip = ip[None, :]\n",
    "print(ip.shape)\n",
    "output = model(ip)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "279c2c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=0.002\n",
    "no_layers = 2\n",
    "vocab_size = len(train_corpus)\n",
    "embedding_dim = 64\n",
    "hidden_dim = 128\n",
    "\n",
    "model = SentimentRNN(vocab_size, embedding_dim, hidden_dim, no_layers)\n",
    "\n",
    "#moving to gpu\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# device = \"cpu\"\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# function to predict accuracy\n",
    "def acc(pred,label):\n",
    "    pred = torch.round(pred.squeeze())\n",
    "    return torch.sum(pred == label.squeeze()).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c622c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 15\n",
    "valid_loss_min = np.Inf\n",
    "batch_size = 16\n",
    "\n",
    "epoch_tr_loss, epoch_vl_loss = [],[]\n",
    "epoch_tr_acc, epoch_vl_acc = [],[]\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_losses = []\n",
    "    train_acc = 0.0\n",
    "    corr = 0\n",
    "    tot = 0\n",
    "\n",
    "    corrval = 0\n",
    "    totval = 0\n",
    "    model.train()\n",
    "    # initialize hidden state \n",
    "    for inputs, labels in trainloader:\n",
    "        labels = labels.to(torch.int64)\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        model.zero_grad()\n",
    "\n",
    "        output = model(inputs)\n",
    "        # calculate the loss and perform backprop\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        train_losses.append(loss.item())\n",
    "        optimizer.step()\n",
    "        preds = torch.argmax(output, 1)\n",
    "        corr += (preds == labels).sum().item()\n",
    "        tot += 16\n",
    "\n",
    "    for inputs, labels in validloader:\n",
    "        labels = labels.to(torch.int64)\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        output = model(inputs)\n",
    "        epoch_val_loss = criterion(output, labels)\n",
    "        # calculate the loss and perform backprop\n",
    "        preds = torch.argmax(output, 1)\n",
    "        corrval += (preds == labels).sum().item()\n",
    "        totval += 16\n",
    "\n",
    "    epoch_train_loss = np.mean(train_losses)\n",
    "    print(f'Epoch {epoch+1}')\n",
    "    print(f'train_loss : {epoch_train_loss}')\n",
    "    print(f'train_accuracy : {corr/tot*100}')\n",
    "    print(f\"valid accuracy: {corrval/totval*100}\")\n",
    "    \n",
    "    if epoch_val_loss <= valid_loss_min:\n",
    "        torch.save(model.state_dict(), '../working/state_dict.pt')\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,epoch_val_loss))\n",
    "        valid_loss_min = epoch_val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b70006be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\andy8\\anaconda3\\envs\\stock_trade\\lib\\site-packages\\pandas\\core\\ops\\array_ops.py:75: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  result = libops.scalar_compare(x.ravel(), y, op)\n",
      "C:\\Users\\andy8\\AppData\\Local\\Temp\\ipykernel_7992\\3237871160.py:65: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Tokens'] = df[\"Tokens\"].apply(pad)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test accuacy is: 0.7876227614095899\n"
     ]
    }
   ],
   "source": [
    "testdf = pd.read_csv(\"twitter_validation.csv\", header = None)\n",
    "testdf = testdf.drop([0, 1], axis = 1)\n",
    "dtestdff = testdf.applymap(str)\n",
    "testdf.columns = [\"Sentiment\", \"Text\"]\n",
    "testdf[\"Target\"] = testdf[\"Sentiment\"].map({\"Negative\" : 0, \"Positive\" : 1, \"Neutral\" : 2, \"Irrelevant\" : 3})\n",
    "\n",
    "df_processed_test = process_data(testdf)\n",
    "\n",
    "max_len = 35\n",
    "testdata, testlens = tokenize(df_processed_test, train_corpus, 35)\n",
    "\n",
    "testX = np.stack(testdata[\"Tokens\"])\n",
    "testY = np.array(testdata[\"Target\"])\n",
    "torchtest = TensorDataset(torch.from_numpy(trainX).to(torch.int64), torch.from_numpy(trainY).to(torch.int64))\n",
    "\n",
    "testloader = DataLoader(torchtest, shuffle=True, batch_size=16)\n",
    "\n",
    "corrval = 0\n",
    "totval = 0\n",
    "\n",
    "for inputs, labels in testloader:\n",
    "    labels = labels.to(torch.int64)\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "    output = model(inputs)\n",
    "    # calculate the loss and perform backprop\n",
    "    preds = torch.argmax(output, 1)\n",
    "    corrval += (preds == labels).sum().item()\n",
    "    totval += 16\n",
    "\n",
    "print(f\"The test accuacy is: {corrval/totval}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8279553e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(sentence, model, tweet_corpus, max_len):\n",
    "    sentence = sentence.lower()\n",
    "    words = sentence.split(\" \")\n",
    "    corpus_dict = corpora.Dictionary([tweet_corpus]).token2id\n",
    "    tokens = [corpus_dict[x] for x in words if x in corpus_dict]\n",
    "    if len(tokens) <= 1:\n",
    "        print(\"No Valid Strings!\")\n",
    "        return None\n",
    "    else:\n",
    "        tokens = np.array(tokens)\n",
    "\n",
    "    if len(tokens) < max_len:\n",
    "        tokens = np.append(tokens, [0]*(max_len - len(tokens)))\n",
    "        tokens = tokens[0:max_len]\n",
    "    \n",
    "    tokens = torch.LongTensor(tokens[None, :])\n",
    "    tokens = tokens.to(device)\n",
    "    output = model(tokens)\n",
    "    preds = torch.argmax(output, 1)\n",
    "    return preds\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stock_trade",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
